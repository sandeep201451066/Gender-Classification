{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "#utilities help us transform our data\n",
    "from keras.utils import * \n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "#from utils import preprocess_input\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data_Generator:\n",
    "    def __init__(self, metadata_path, batch_size, val_split):\n",
    "\n",
    "        self.data = scipy.io.loadmat(metadata_path)\n",
    "        self.img_paths = self.data['wiki']['full_path'][0][0][0]\n",
    "        self.genders = self.data['wiki']['gender'][0][0][0]\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split  \n",
    "        self.face_score = self.data['wiki']['face_score'][0][0][0]\n",
    "        self.second_face_score = self.data['wiki']['second_face_score'][0][0][0]\n",
    "        self.load_keys()\n",
    "\n",
    "    def load_keys(self):\n",
    "        \n",
    "        \"\"\"     load wiki dataset \n",
    "        #       Take 5000 images of males and females\n",
    "                Faces are selected with some threshold\n",
    "                As all images in database are not accurate\n",
    "                for gender \"0\" represents female and \"1\" represents male and \"nan\" represents blank image                \n",
    "        \"\"\" \n",
    "        indices = []\n",
    "        count_male, count_female = 0, 0\n",
    "        n_m, n_f = 5000,5000\n",
    "        for i in range(len(self.img_paths)):\n",
    "            if count_female == n_f and count_male == n_m:\n",
    "                break            \n",
    "            if (self.face_score[i] > 3.00000000000 and (math.isnan(self.second_face_score[i]) == True)):\n",
    "                if (math.isinf(self.face_score[i]) == False):\n",
    "                    if (math.isnan(self.genders[i]) == False):\n",
    "                        gender = self.genders[i]\n",
    "                        if gender == 0 and count_female < n_f:\n",
    "                            count_female += 1\n",
    "                            indices.append(i)\n",
    "                        elif gender == 1 and count_male < n_m:\n",
    "                            count_male += 1\n",
    "                            indices.append(i)  \n",
    "                               \n",
    "        self.number_of_imgs = len(indices)\n",
    "\n",
    "        self.val_size = int(self.number_of_imgs * self.val_split)\n",
    "        self.train_size = self.number_of_imgs - self.val_size\n",
    "        shuffle(indices)\n",
    "        self.train_keys = indices[:self.train_size]\n",
    "        self.val_keys = indices[self.train_size:]\n",
    "\n",
    "\n",
    "    def load_data(self, is_train):\n",
    "        \n",
    "        \"\"\"     load wiki dataset \n",
    "        #       Returns: faces and genders\n",
    "                face: shape (64, 64, 1)\n",
    "                gender_labels: 0 for female and 1 for male\n",
    "        \"\"\" \n",
    "        \n",
    "        while 1:\n",
    "            faces = []\n",
    "            gender_labels = []   \n",
    "\n",
    "            shuffle(self.train_keys)\n",
    "            keys = self.train_keys\n",
    "            if not is_train:\n",
    "                keys = self.val_keys\n",
    "\n",
    "            for key in keys:\n",
    "                img_path = self.img_paths[key][0]\n",
    "                img = cv2.imread('./crop_img/' + img_path, 0)            \n",
    "\n",
    "                faces.append(cv2.resize(img, (64, 64)))\n",
    "                gender_labels.append(self.genders[key])\n",
    "\n",
    "                if len(faces) == self.batch_size:\n",
    "                    faces = np.expand_dims(faces,-1)\n",
    "                    gender_labels = pd.get_dummies(gender_labels).as_matrix()\n",
    "                    yield (faces, gender_labels)\n",
    "                    faces = []\n",
    "                    gender_labels = []\n",
    "\n",
    "            if len(faces) == 0:\n",
    "                continue\n",
    "            faces = np.expand_dims(faces,-1)\n",
    "            gender_labels = pd.get_dummies(gender_labels).as_matrix()\n",
    "            yield (faces, gender_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './crop_img/wiki.mat'\n",
    "model_save_path = './crop_img/test_CNN_model.hdf5'\n",
    "num_classes = 2\n",
    "image_size = (64, 64, 1)\n",
    "batch_size = 150\n",
    "num_epochs = 20\n",
    "val_split = 0.1\n",
    "data = Data_Generator(data_path, batch_size, val_split)\n",
    "gen_train = data.load_data(True)\n",
    "gen_val = data.load_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model structure\n",
    "\n",
    "def CNN(input_shape,num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(16, 7, 7, border_mode='same',\n",
    "                            input_shape=input_shape))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(5, 5),strides=(2, 2), border_mode='same'))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Convolution2D(32, 5, 5, border_mode='same'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3),strides=(2, 2), border_mode='same'))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(AveragePooling2D(pool_size=(3, 3),strides=(2, 2), border_mode='same'))\n",
    "    model.add(Dropout(.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1028))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1028))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model and compile it\n",
    "\n",
    "model = CNN(image_size, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "                                        metrics=['accuracy'])\n",
    "csv_logger = CSVLogger('training.log')\n",
    "early_stop = EarlyStopping('val_acc', patience=200, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(model_save_path,\n",
    "                                    'val_acc', verbose=0,\n",
    "                                    save_best_only=True)\n",
    "\n",
    "model_callbacks = [early_stop, model_checkpoint, csv_logger]\n",
    "\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "model.fit_generator(gen_train, nb_epoch=num_epochs, verbose=1, \n",
    "                                    validation_data=gen_val,\n",
    "                                    samples_per_epoch=data.train_size,\n",
    "                                    nb_val_samples=data.val_size,\n",
    "                                    callbacks=model_callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
